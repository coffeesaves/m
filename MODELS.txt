import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load dataset
dataset = pd.read_csv('your_dataset.csv')

# Split dataset into features and target variable
X = dataset.drop('target_column', axis=1)
y = dataset['target_column']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 1. Linear Regression, Lasso, Ridge
linear_reg = LinearRegression()
lasso = Lasso()
ridge = Ridge()

# Fit models
linear_reg.fit(X_train_scaled, y_train)
lasso.fit(X_train_scaled, y_train)
ridge.fit(X_train_scaled, y_train)

# Predictions
linear_reg_preds = linear_reg.predict(X_test_scaled)
lasso_preds = lasso.predict(X_test_scaled)
ridge_preds = ridge.predict(X_test_scaled)

# Evaluation
print("Linear Regression:")
print("R^2 Score:", r2_score(y_test, linear_reg_preds))
print("Mean Absolute Error:", mean_absolute_error(y_test, linear_reg_preds))
print("Mean Squared Error:", mean_squared_error(y_test, linear_reg_preds))


print("Lasso:")
print("Mean Squared Error:", mean_squared_error(y_test, lasso_preds))
print("R^2 Score:", r2_score(y_test, lasso_preds))

print("Ridge:")
print("Mean Squared Error:", mean_squared_error(y_test, ridge_preds))
print("R^2 Score:", r2_score(y_test, ridge_preds))

# 2. Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
decision_tree_preds = decision_tree.predict(X_test)

# Overfitting graph
train_errors = []
test_errors = []
for depth in range(1, 20):
    dt = DecisionTreeClassifier(max_depth=depth)
    dt.fit(X_train, y_train)
    train_errors.append(1 - dt.score(X_train, y_train))
    test_errors.append(1 - dt.score(X_test, y_test))
    
plt.plot(range(1, 20), train_errors, label='Train')
plt.plot(range(1, 20), test_errors, label='Test')
plt.xlabel('Max Depth of Decision Tree')
plt.ylabel('Error')
plt.legend()
plt.show()

# Evaluation
print("Decision Tree:")
print("Accuracy Score:", accuracy_score(y_test, decision_tree_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, decision_tree_preds))
print("Classification Report:\n", classification_report(y_test, decision_tree_preds))

# 3. Naive Bayes
naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)
naive_bayes_preds = naive_bayes.predict(X_test)

# Evaluation
print("Naive Bayes:")
print("Accuracy Score:", accuracy_score(y_test, naive_bayes_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, naive_bayes_preds))
print("Classification Report:\n", classification_report(y_test, naive_bayes_preds))

# 4. Logistic Regression with Hyperparameter Tuning
logistic_reg = LogisticRegression()
params = {'C': [0.1, 1, 10, 100]}
logistic_reg_grid = GridSearchCV(logistic_reg, params, cv=5)
logistic_reg_grid.fit(X_train_scaled, y_train)
logistic_reg_best = logistic_reg_grid.best_estimator_
logistic_reg_best.fit(X_train_scaled, y_train)
logistic_reg_preds = logistic_reg_best.predict(X_test_scaled)

# Evaluation
print("Logistic Regression:")
print("Accuracy Score:", accuracy_score(y_test, logistic_reg_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, logistic_reg_preds))
print("Classification Report:\n", classification_report(y_test, logistic_reg_preds))

# 5. SVM Linear and Kernel with Hyperparameter Tuning
svm_linear = SVC(kernel='linear')
svm_kernel = SVC(kernel='rbf')
params_svm = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001]}
svm_grid = GridSearchCV(svm_kernel, params_svm, cv=5)
svm_grid.fit(X_train_scaled, y_train)
svm_best = svm_grid.best_estimator_
svm_best.fit(X_train_scaled, y_train)
svm_preds = svm_best.predict(X_test_scaled)

# Evaluation
print("SVM:")
print("Accuracy Score:", accuracy_score(y_test, svm_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, svm_preds))
print("Classification Report:\n", classification_report(y_test, svm_preds))

# 6. Random Forest with Feature Importance
random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)
feature_importances = random_forest.feature_importances_

#Build Random Forest Classifier without Feature Selection
clf2 = RandomForestClassifier(n_estimators=100, random_state=42)
clf2.fit(X_train2, y_train2)
y_pred2 = clf2.predict(X_test2)

# Calculate Accuracy
acc_no_selection2 = accuracy_score(y_test2, y_pred2)
print("Accuracy without Feature Selection on Dataset 2:", acc_no_selection2)

# Evaluation
print("Random Forest:")
print("Accuracy Score:", accuracy_score(y_test, random_forest.predict(X_test)))
print("Feature Importances:", feature_importances)

# Perform Feature Selection
feature_imp2 = pd.Series(clf2.feature_importances_, index=X2.columns).sort_values(ascending=False)
selected_features2 = feature_imp2[feature_imp2 > 0.2].index  # Example threshold for selection

# Build Random Forest Classifier with Feature Selection
clf_selected2 = RandomForestClassifier(n_estimators=100, random_state=42)
clf_selected2.fit(X_train2[selected_features2], y_train2)
y_pred_selected2 = clf_selected2.predict(X_test2[selected_features2])

# Calculate Accuracy
acc_with_selection2 = accuracy_score(y_test2, y_pred_selected2)
print("Accuracy with Feature Selection on Dataset 2:", acc_with_selection2)

sns.barplot(x=feature_imp2, y=feature_imp2.index)

# 7. AdaBoost with Hyperparameter Tuning
adaboost = AdaBoostClassifier()
params_adaboost = {'n_estimators': [50, 100, 200, 300], 'learning_rate': [0.01, 0.1, 1]}
adaboost_grid = GridSearchCV(adaboost, params_adaboost, cv=5)
adaboost_grid.fit(X_train, y_train)
adaboost_best = adaboost_grid.best_estimator_
adaboost_best.fit(X_train, y_train)
adaboost_preds = adaboost_best.predict(X_test)

# Evaluation
print("AdaBoost:")
print("Accuracy Score:", accuracy_score(y_test, adaboost_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, adaboost_preds))
print("Classification Report:\n", classification_report(y_test, adaboost_preds))

# 8. XGBoost with Hyperparameter Tuning
xgb = XGBClassifier()
params_xgb = {'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 5, 7, 9]}
xgb_grid = GridSearchCV(xgb, params_xgb, cv=5)
xgb_grid.fit(X_train, y_train)
xgb_best = xgb_grid.best_estimator_
xgb_best.fit(X_train, y_train)
xgb_preds = xgb_best.predict(X_test)

# Evaluation
print("XGBoost:")
print("Accuracy Score:", accuracy_score(y_test, xgb_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, xgb_preds))
print("Classification Report:\n", classification_report(y_test, xgb_preds))

from sklearn.metrics import silhouette_score, adjusted_rand_score

# 9. KMeans Clustering with Pipeline

from sklearn.pipeline import make_pipeline
kmeans = KMeans(n_clusters=2)
pipeline = make_pipeline(scaler, kmeans)
pipeline.fit(X_train)
cluster_preds = pipeline.predict(X_test)

# Evaluation
silhouette = silhouette_score(X_test_scaled, cluster_preds)
adjusted_rand = adjusted_rand_score(y_test, cluster_preds)
print("KMeans Clustering:")
print("Silhouette Score:", silhouette)
print("Adjusted Rand Score:", adjusted_rand)


plt.plot(range(1, 11), sse, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.title('Elbow Method')
plt.show()

#silhouette
kmeans_kwargs = {"init": "random", "n_init": 10, "max_iter": 300, "random_state": 42}
silhouette_coef = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(features)
    score = silhouette_score(features, kmeans.labels_)
    silhouette_coef.append(score)

plt.figure(figsize=(6, 6))
plt.plot(range(2, 11), silhouette_coef, '-o')
plt.xlabel(xlabel="Number of clusters (k)")
plt.ylabel(ylabel="Silhouette Coefficient")
plt.title(label="Silhouette Method")
plt.show()


#elbow
list_k = list(range(2, 10))
sse = []
for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(features)
    sse.append(km.inertia_)

plt.figure(figsize=(6, 6))
plt.plot(list_k, sse, '-o')
plt.xlabel(xlabel="Number of clusters (k)")
plt.ylabel(ylabel="SSE")
plt.title(label="Elbow Method")
plt.show()



# 10. Comparison of any two algorithms on the same dataset
# Let's compare Logistic Regression and Random Forest
logistic_reg_score = accuracy_score(y_test, logistic_reg_preds)
random_forest_score = accuracy_score(y_test, random_forest.predict(X_test))
print("Logistic Regression Accuracy:", logistic_reg_score)
print("Random Forest Accuracy:", random_forest_score)

# 11. Model Building with Cross Validation
# Using cross_val_score
scores = cross_val_score(logistic_reg, X, y, cv=5)
print("Cross-validated Logistic Regression scores:", scores)

# 12. Linear Regression from Scratch
class LinearRegressionScratch:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.train_errors = []
        self.test_errors = []
        
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for _ in range(self.n_iterations):
            y_predicted = np.dot(X, self.weights) + self.bias
            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1/n_samples) * np.sum(y_predicted - y)
            
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Calculate training error
            train_error = np.mean((y_predicted - y) ** 2)
            self.train_errors.append(train_error)
            
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# Usage:
linear_reg_scratch = LinearRegressionScratch()
linear_reg_scratch.fit(X_train_scaled, y_train)
linear_reg_scratch_preds = linear_reg_scratch.predict(X_test_scaled)

# Evaluation
print("Linear Regression from Scratch:")
print("Mean Squared Error:", mean_squared_error(y_test, linear_reg_scratch_preds))
print("R^2 Score:", r2_score(y_test, linear_reg_scratch_preds))


#DIFFERENT EVALUATION METHODS


from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc

# Confusion Matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

# Precision-Recall Curve
def plot_precision_recall_curve(y_true, y_prob):
    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
    plt.plot(recall, precision, marker='.')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.show()

# AUC and ROC Curve
def plot_roc_curve(y_true, y_prob):
    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
    plt.plot(fpr, tpr, marker='.')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.show()

# Mean Squared Error
def calculate_mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

# SSE for KMeans
def calculate_sse(kmeans_model, X):
    return kmeans_model.inertia_

# Evaluation for Linear Regression, Logistic Regression, SVM, Random Forest, AdaBoost, XGBoost
def evaluate_classification(y_true, y_pred, y_prob):
    print("Confusion Matrix:")
    plot_confusion_matrix(y_true, y_pred)
    print("Classification Report:")
    print(classification_report(y_true, y_pred))
    print("AUC Score:", roc_auc_score(y_true, y_prob))
    print("ROC Curve:")
    plot_roc_curve(y_true, y_prob)
    print("Precision-Recall Curve:")
    plot_precision_recall_curve(y_true, y_prob)


